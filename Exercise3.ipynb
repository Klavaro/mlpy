{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 3: Neural Networks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import Exercise3helper37 as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(h,a=1):\n",
    "    return 1/(1+np.exp(-a*h))\n",
    "def dlogistic(h,a=1):\n",
    "    return logistic(h,a)-np.power(logistic(h,a),2)\n",
    "\n",
    "signtrafunc=lambda x: (np.sign(x)+1)/2\n",
    "\n",
    "def gaussRBF(h,a=1):\n",
    "    return np.exp(-(a*h)**2)\n",
    "\n",
    "def dgaussRBF(h,a=1):\n",
    "    return -2*a*h*np.exp(-(a*h)**2)\n",
    "\n",
    "class neuron:    \n",
    "    \n",
    "    def __init__(self,w,b=0,trafunc=logistic,dtrafunc=dlogistic):\n",
    "        self.w=np.array(w)\n",
    "        self.b=np.array(b)\n",
    "        self.trafunc=trafunc\n",
    "        self.dtrafunc=dtrafunc\n",
    "        \n",
    "    def out(self, x):\n",
    "        return self.trafunc(np.dot(self.w,x)-self.b)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPlayer:\n",
    "    def __init__(self,NodeNo,WeightNo,weightinit=np.random.randn,biasinit=np.random.randn,trafunc=logistic,dtrafunc=dlogistic):\n",
    "        self.nodes=[neuron(weightinit(WeightNo),biasinit(1),trafunc,dtrafunc) for i in range(NodeNo)]\n",
    "    def out(self,x):\n",
    "        return np.ravel([n.out(x) for n in self.nodes])\n",
    "    def train(self,deltanext,W,learnrate=0.1):    \n",
    "        Wo=np.array([n.w for n in self.nodes]).T\n",
    "        deltas=np.array([ n.train(deltanext,W[ineur],learnrate) for ineur,n in enumerate(self.nodes)]).T\n",
    "        return deltas, Wo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Backpropagation in MLPs (5 points - programming)\n",
    "\n",
    "Construct a class \"MLP\" with a list of layers of type \"MLPlayer\" called \"MLP.layers\" Also, there should be a method \"MLP.out(x)\" that returns the outputs of the whole network of the input vector \"x\".\n",
    "\n",
    "Make sure, that the size of the weight vector is set to the number of inputs for the first layer and that the number of inputs for the following layers has to correspond with the number of neurons in the preceding layer. The number of outputs equals the number of neurons in the last layer.\n",
    "\n",
    "* The init function takes the number of inputs *x* as an integer and the number of nodes for each layer as a list. The number of neurons per layer and the number of inputs should be passed to the initialization methods of an MLPlayer.\n",
    "\n",
    "* Include the backpropagation training algorithm as a method \"MLP.train()\" into the class. The passed arguments should consist of the number of iterations (no stopping criteria in this case), the training input and the training output - both as function pointers - as well as the learning rate. It should iterate over the layers, which themselves iterate over their neurons. Deltas and W of the following layer will allways be the input to the previous.\n",
    "\n",
    "* The function x_train has to produce a random array of inputs of size [NoInputs,] and o_train has to produce the corresponding target function output. x_train should work without an argument passed and the target training output $o$ should be calculated using only that vector $x$.\n",
    "\n",
    "* The output of the method *train()* should consist of the sum-of-squares error within each iteration.\n",
    "\n",
    "Do the same training on an XOR as in exercise sheet 2 task 5, using an architecture of 2 and then 3 neurons in the first layer and one output neuron in the second (you may use the sample solution code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, input_num, node_num, weight, bias, trafunc = logistic, dtrafunc = dlogistic):\n",
    "        self.weight =np.random.randn\n",
    "        self.bias =np.random.randn\n",
    "        self.node_num =np.append(input_num, node_num)\n",
    "        self.layers =[]\n",
    "        for i in range(self.node_num.size - 1):\n",
    "            self.layers += MLPlayer(self.node_num[i + 1], self.node_num[i], weight, bias, trafunc, dtrafunc)\n",
    "\n",
    "    def out(self, x):\n",
    "        ret =self.layers[0].out(x)\n",
    "        for i in range(1,len(self.layers)):\n",
    "            ret =self.layers[i].out(ret)\n",
    "        self.lastout =ret\n",
    "        return self.lastout\n",
    "\n",
    "    def train(self, iterations, x_train, o_train, learnrate = 0.1):\n",
    "        err = np.zeros(iterations)\n",
    "        for i in range(iterations):\n",
    "            if callable(x_train):\n",
    "                x =x_train()\n",
    "            else:\n",
    "                x =x_train[i]\n",
    "            if callable(o_train):\n",
    "                o =o_train(x)\n",
    "            else:\n",
    "                o =o_train[i]\n",
    "                \n",
    "            y = self.out(x)\n",
    "            \n",
    "            if isinstance(y,np.array)==false:\n",
    "                o =np.array([o])\n",
    "                y =np.array([y])\n",
    "                \n",
    "            deltas =y - o\n",
    "            \n",
    "            W = np.eye(len(y))\n",
    "            \n",
    "            for inp,nod in enumerate(reversed(self.layers)):\n",
    "                deltas, W =nod.train(deltas, W, learnrate)\n",
    "\n",
    "            err[i] =np.sum((y - o)**2)/2\n",
    "\n",
    "        return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Lateral Inhibition  (3  points - programming)\n",
    "Build a network layer representing the simple lateral inhibition we have faced in lecture 2. This involves only a direct neighbour supression. It represents a not fully-connected MLP layer. The layer should be 10 neurons wide with the same number of inputs.\n",
    "Set the bias to $b=0$, the weights for the corresponding center input to 1 and the direct neighbour weights to $w=-0.25$. Later, repeat the simulation with $w=-0.5$.\n",
    "\n",
    "As inputs, use a random vector of 0 and 1 (np.random.randint).\n",
    "\n",
    "First solve the task with the logistic transfer function. Then use the sign-based transfer function and examine the results with plots of the input together with the corresponding output over their indices.\n",
    "\n",
    "No training, deltas etc. are neccessary for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LateralInhibitionLayer:\n",
    "    nodes = []\n",
    "\n",
    "    def __init__(self, NodeNo, Neighbourweight, bias = 0, trafunc = logistic):\n",
    "        \n",
    "        weights = np.eye(NodeNo) + (np.eye(NodeNo,k=1)*Neighbourweight) + (np.eye(NodeNo,k=-1)*Neighbourweight)\n",
    "        self.nodes=[]\n",
    "        for i in range(NodeNo):\n",
    "            self.nodes+=neuron(weights[i, :], bias, trafunc, dtrafunc)\n",
    "\n",
    "    def out(self, x):\n",
    "        ret=[]\n",
    "        for n in self.nodes:\n",
    "            ret+=n.out(x)\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Lateral Inhibition: trained (2 points - programming)\n",
    "Use the output of the lateral inhibition layer from task 1 as a target output for an MLP with 1 layer of 10 neurons.\n",
    "Try out both tranfer functions for the lateral inhibition layer (target) but only the logistic for the MLP (the one we are training). Use $w=-0.25$ for the inhibition of the lateral inhibition layer.\n",
    "\n",
    "As inputs, use again a random vector of 0 and 1 (np.random.randint).\n",
    "\n",
    "Determine a number of iterations that you decide to be enough for the network to have converged.\n",
    "\n",
    "Measure the execution time by using the python module time and time.time() to get the current time to compare it later to the one from task 4. Also, investigate the weights after initialization and after the last iteration.\n",
    "\n",
    "If you haven't solved task 1, you can use the class *LateralInhibitionLayer(NodeNo,Neighbourweight,bias=0,trafunc=logistic)* from the Exercise3Helper with the method LateralInhibitionLayer.out(x) as the target function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Training a 1D-Convolutional NN on lateral inhibition (2 points - programming)\n",
    "\n",
    "Use the output of the lateral inhibition layer as a target output for a CNN consisting of 1 layer with 10 neurons. The code for the CNNlayer class is given below. You can simnply abuse the MLP class and assign a single layer using the CNNlayer class with 3 weights (see code below). Use $w=-0.25$ for the inhibition in the lateral inhibiton layer.\n",
    "\n",
    "As inputs, use again a random vector of 0 and 1 (np.random.randint).\n",
    "\n",
    "Measure and compare the execution time to the one of the MLP trained on the same problem (Task 2) by using the python module time and time.time() to get the current time.  Use the same number of intializations. Also, investigate the weights after initialization and after the last iteration.\n",
    "\n",
    "If you haven't solved task 2, you can use the class *LateralInhibitionLayer(NodeNo,Neighbourweight,bias=0,trafunc=logistic)* from the Exercise3Helper with the method LateralInhibitionLayer.out(x) as the target function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNlayer:\n",
    "    def __init__(self,Filterwidth,weightinit=np.random.randn,biasinit=np.random.randn,trafunc=logistic,dtrafunc=dlogistic):        \n",
    "        #self.nodes=[neuron(weightinit(WeightNo),biasinit(1),trafunc) for i in range(NodeNo)]\n",
    "        self.w=weightinit(Filterwidth) \n",
    "        self.b=biasinit(1)\n",
    "        self.trafunc=trafunc\n",
    "        self.dtrafunc=dtrafunc\n",
    "        \n",
    "    def out(self,x):\n",
    "        self.lastin=x;\n",
    "        self.lasth=np.convolve(x,self.w,mode='same')-self.b\n",
    "        self.lastout=self.trafunc(self.lasth)\n",
    "        return self.lastout\n",
    "    \n",
    "    def delta(self,deltanext,weightsnext):\n",
    "        df=self.dtrafunc(self.lasth)\n",
    "        self.lastdelta=np.dot(deltanext,weightsnext)*df\n",
    "        return self.lastdelta\n",
    "    \n",
    "    def train(self,deltanext,W,learnrate=0.1):        \n",
    "        self.delta(deltanext,W)\n",
    "        WE=np.eye(len(self.lastin))\n",
    "        W=np.array([np.convolve(we,self.w,mode='same') for we in WE])\n",
    "        self.w=np.array([self.w[i]-learnrate*np.sum(self.lastdelta*np.convolve(self.lastin,np.eye(len(self.w),1,-i).ravel(),mode='same')) for i in range(len(self.w))])\n",
    "        self.b=self.b+learnrate*np.sum(self.lastdelta)        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LayerNos=[10]\n",
    "CNN=helper.MLP(10,LayerNos)\n",
    "CNN.layers[0]=helper.CNNlayer(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Local Oscilations (3 points - programming)\n",
    "Rewrite the neuron class below to receive a neuron with local feedback capable of oscillations. This will be a very simple example to demonstrate oscillations, biologically not very plausible as we e.g. don't include the refactory period.\n",
    "\n",
    "To this extent, you need to save and feedback the last output in a certrain time interval. No training or delta functions are necessary.  Use the logistic function as the transfer function. The output function becomes:\n",
    "$y(t)=f(w\\cdot x(t)+w_\\tau y(t-\\tau)-b)$\n",
    "\n",
    "Initialize a regular perceptron and a local feedback neuron with the same random weights. The input should be of shape (2,), randomly distributed with an average of 0.5 and a standard deviation of 0.2.\n",
    "Repeat 200 iterations each with every possible combination of the following parameters: $\\tau=\\{10,5\\}$ samples and $w_\\tau=\\{10,5,1,-1,-5,-10\\}$ using exactly the same random input for each parameter set and interation.\n",
    "\n",
    "Plot the output compared for every single parameter set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_taus=[10,5,1,-1,-5,-10]\n",
    "taus=[10,5]\n",
    "numIt=200\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
